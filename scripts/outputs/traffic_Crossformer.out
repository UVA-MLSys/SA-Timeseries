Namespace(task_name='long_term_forecast', train=False, model='Crossformer', seed=2024, itrs=3, itr_no=None, data='custom', result_path='./results', root_path='./dataset/traffic/', data_path='traffic.csv', flag='test', features='S', target='OT', freq='h', seq_len=96, label_len=12, pred_len=24, seasonal_patterns='Monthly', top_k=5, num_kernels=6, n_features=1, d_model=128, n_heads=4, e_layers=2, d_layers=1, d_ff=256, moving_avg=25, factor=3, distil=True, dropout=0.1, embed='timeF', activation='gelu', output_attention=False, conv_kernel=[18, 12], seg_len=24, num_workers=0, train_epochs=10, batch_size=32, patience=3, learning_rate=0.001, lradj='type1', use_amp=False, use_cpu=False, gpu=0, use_multi_gpu=False, devices='0,1,2,3', p_hidden_dims=[128, 128], p_hidden_layers=2, explainers=['feature_ablation', 'occlusion', 'augmented_occlusion', 'winIT', 'tsr'], areas=[0.05, 0.075, 0.1, 0.15], baseline_mode='random', metrics=['mae', 'mse'], overwrite=True, dump_attrs=False, disable_progress=True, use_gpu=True, enc_in=1, dec_in=1, c_out=1)

>>>> itr_no: 1, seed: 648 <<<<<<
Use GPU: cuda:0
Experiments will be saved in ./results/traffic_Crossformer/1
test 1731
Loading model from ./results/traffic_Crossformer/1/checkpoint.pth
Running feature_ablation from 2024-07-08 01:50:25.197947
Experiment ended at 2024-07-08 03:26:37.321209. Total time taken 1:36:12.123262.
  metric   area      comp       suff
0    mae  0.050  6.638271  21.656200
1    mae  0.075  7.795961  18.944777
2    mae  0.100  8.328426  17.431671
3    mae  0.150  9.448458  14.344310
4    mse  0.050  3.590200  27.975851
5    mse  0.075  4.621412  22.067965
6    mse  0.100  5.143127  19.030875
7    mse  0.150  6.358459  13.412906

Running occlusion from 2024-07-08 03:26:37.529687
Experiment ended at 2024-07-08 03:36:41.122580. Total time taken 0:10:03.592893.
  metric   area      comp       suff
0    mae  0.050  6.677005  21.718502
1    mae  0.075  7.867679  18.984480
2    mae  0.100  8.425278  17.470473
3    mae  0.150  9.553806  14.399225
4    mse  0.050  3.645206  28.202328
5    mse  0.075  4.710473  22.229727
6    mse  0.100  5.258837  19.177664
7    mse  0.150  6.514442  13.543144

Running augmented_occlusion from 2024-07-08 03:36:41.292861
Experiment ended at 2024-07-08 03:46:45.479762. Total time taken 0:10:04.186901.
  metric   area      comp       suff
0    mae  0.050  4.715859  24.152871
1    mae  0.075  5.808223  22.335018
2    mae  0.100  6.336702  21.207210
3    mae  0.150  7.358132  18.678604
4    mse  0.050  2.125495  34.137497
5    mse  0.075  2.885366  29.677139
6    mse  0.100  3.298020  27.056926
7    mse  0.150  4.182856  21.555329

Running winIT from 2024-07-08 03:46:45.649463
Traceback (most recent call last):
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/interpret.py", line 73, in <module>
    main(args)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/interpret.py", line 42, in main
    interpreter.interpret(dataloader)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/exp/exp_interpret.py", line 185, in interpret
    results, attrs = self.run_regressor(dataloader, name)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/exp/exp_interpret.py", line 159, in run_regressor
    batch_results, batch_attr = self.evaluate(
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/exp/exp_interpret.py", line 210, in evaluate
    attr = compute_attr(
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/utils/explainer.py", line 103, in compute_attr
    attr = explainer.attribute(
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/utils/winIT.py", line 140, in attribute
    return self._attribute_tuple(
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/utils/winIT.py", line 188, in _attribute_tuple
    model(*tuple(inputs_hat), *additional_forward_args)
  File "/home/mi3se/.conda/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/models/Crossformer.py", line 135, in forward
    dec_out = self.forecast(x_enc, x_mark_enc, x_dec, x_mark_dec)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/models/Crossformer.py", line 92, in forecast
    dec_out = self.decoder(dec_in, enc_out)
  File "/home/mi3se/.conda/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/layers/Crossformer_EncDec.py", line 122, in forward
    x, layer_predict = layer(x, cross_enc)
  File "/home/mi3se/.conda/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/layers/Crossformer_EncDec.py", line 92, in forward
    x = self.self_attention(x)
  File "/home/mi3se/.conda/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/layers/SelfAttention_Family.py", line 294, in forward
    dim_receive, attn = self.dim_receiver(dim_send, dim_buffer, dim_buffer, attn_mask=None, tau=None, delta=None)
  File "/home/mi3se/.conda/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/sfs/gpfs/tardis/project/bii_dsc_community/khairul/SA-Timeseries/layers/SelfAttention_Family.py", line 200, in forward
    keys = self.key_projection(keys).view(B, S, H, -1)
  File "/home/mi3se/.conda/envs/ml/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1194, in _call_impl
    return forward_call(*input, **kwargs)
  File "/home/mi3se/.conda/envs/ml/lib/python3.9/site-packages/torch/nn/modules/linear.py", line 114, in forward
    return F.linear(input, self.weight, self.bias)
torch.cuda.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 MiB (GPU 0; 31.73 GiB total capacity; 31.17 GiB already allocated; 5.75 MiB free; 31.19 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF
