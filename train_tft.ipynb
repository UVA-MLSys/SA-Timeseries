{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, gc, torch\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pandas as pd\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.data import GroupNormalizer, MultiNormalizer\n",
    "from pytorch_forecasting.metrics import RMSE, MultiLoss\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device = torch.device(device)\n",
    "\n",
    "print(f'Using {device} backend.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configurations.config import *\n",
    "@dataclass\n",
    "class arguments:\n",
    "    experiment = 'traffic'\n",
    "    show_progress = True\n",
    "\n",
    "config = ExperimentConfig(experiment=arguments.experiment)\n",
    "formatter = config.data_formatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = formatter.read_file()\n",
    "print(f'Total data shape {df.shape}')\n",
    "\n",
    "from utils.metric import show_result\n",
    "from utils.data import create_TimeSeriesDataSet\n",
    "from utils.model import seed_torch\n",
    "seed_torch(seed=config.seed)\n",
    "train, validation, test = formatter.split(df)\n",
    "\n",
    "parameters = config.model_parameters(ModelType.TFT)\n",
    "batch_size = parameters['batch_size']\n",
    "_, train_dataloader = create_TimeSeriesDataSet(\n",
    "    train, formatter, batch_size, train=True\n",
    ")\n",
    "_, val_dataloader = create_TimeSeriesDataSet(validation, formatter, batch_size)\n",
    "test_timeseries, test_dataloader = create_TimeSeriesDataSet(test, formatter, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "# click this and locate the lightning_logs folder path and select that folder. \n",
    "# this will load tensorbaord visualization\n",
    "import tensorboard as tb\n",
    "tf.io.gfile = tb.compat.tensorflow_stub.io.gfile\n",
    "\n",
    "early_stop_callback = EarlyStopping(\n",
    "    monitor=\"val_loss\", min_delta=0, \n",
    "    patience=parameters['early_stopping_patience']\n",
    "    , verbose=True, mode=\"min\"\n",
    ")\n",
    "best_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=config.experiment_folder, monitor=\"val_loss\", \n",
    "    filename=\"best-{epoch}\"\n",
    ")\n",
    "latest_checkpoint = pl.callbacks.ModelCheckpoint(\n",
    "    dirpath=config.experiment_folder, \n",
    "    every_n_epochs=1, filename=\"latest-{epoch}\"\n",
    ")\n",
    "\n",
    "logger = TensorBoardLogger(config.experiment_folder)  # logging results to a tensorboard\n",
    "\n",
    "# https://pytorch-lightning.readthedocs.io/en/stable/common/trainer.html#trainer-class-api\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = parameters['epochs'],\n",
    "    accelerator = 'auto',\n",
    "    enable_model_summary=True,\n",
    "    callbacks = [early_stop_callback, best_checkpoint, latest_checkpoint],\n",
    "    logger = logger,\n",
    "    enable_progress_bar = arguments.show_progress,\n",
    "    check_val_every_n_epoch = 2,\n",
    "    max_time=pd.to_timedelta(1, unit='minutes')\n",
    ")\n",
    "\n",
    "tft = TemporalFusionTransformer.from_dataset(\n",
    "    test_timeseries,\n",
    "    learning_rate= parameters['learning_rate'],\n",
    "    hidden_size= parameters['hidden_layer_size'],\n",
    "    attention_head_size=parameters['attention_head_size'],\n",
    "    dropout=parameters['dropout_rate'],\n",
    "    loss=MultiLoss([RMSE(reduction='mean') for _ in formatter.targets]), # RMSE(reduction='sqrt-mean')\n",
    "    optimizer='adam',\n",
    "    log_interval=1,\n",
    "    # reduce_on_plateau_patience=2\n",
    ")\n",
    "\n",
    "print(f\"Number of parameters in network: {tft.size()/1e3:.1f}k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "gc.collect()\n",
    "\n",
    "start = datetime.now()\n",
    "print(f'\\n----Training started at {start}----\\n')\n",
    "\n",
    "trainer.fit(\n",
    "    tft,\n",
    "    train_dataloaders=train_dataloader,\n",
    "    val_dataloaders=val_dataloader,\n",
    ")\n",
    "end = datetime.now()\n",
    "print(f'\\n----Training ended at {end}, elapsed time {end-start}')\n",
    "print(f'Best model by validation loss saved at {trainer.checkpoint_callback.best_model_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.PredictionProcessor import PredictionProcessor\n",
    "\n",
    "processor = PredictionProcessor(\n",
    "    formatter.time_index, formatter.group_id, \n",
    "    formatter.parameters['horizon'], formatter.targets, \n",
    "    formatter.parameters['window']\n",
    ")\n",
    "\n",
    "# %%\n",
    "from classes.Plotter import *\n",
    "\n",
    "plotter = PlotResults(\n",
    "   config.experiment_folder, formatter.time_index, \n",
    "   formatter.targets, show=arguments.show_progress\n",
    ")\n",
    "\n",
    "best_model_path = trainer.checkpoint_callback.best_model_path\n",
    "print(f'Loading best model from {best_model_path}')\n",
    "\n",
    "# tft = TemporalFusionTransformer.load_from_checkpoint(best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# print('\\n---Training prediction--\\n')\n",
    "# train_predictions, train_index = tft.predict(\n",
    "#     train_dataloader, return_index=True, \n",
    "#     show_progress_bar=arguments.show_progress\n",
    "# )\n",
    "# train_result_merged = processor.align_result_with_dataset(\n",
    "#    train, train_predictions, train_index\n",
    "# )\n",
    "\n",
    "# show_result(train_result_merged, formatter.targets)\n",
    "# plotter.summed_plot(train_result_merged, type='Train_error', plot_error=True)\n",
    "# gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n---Validation results--\\n')\n",
    "\n",
    "validation_predictions, validation_index = tft.predict(\n",
    "    val_dataloader, return_index=True, \n",
    "    show_progress_bar=arguments.show_progress\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_result_merged = processor.align_result_with_dataset(\n",
    "   validation, validation_predictions, validation_index\n",
    ")\n",
    "show_result(validation_result_merged, formatter.targets)\n",
    "\n",
    "plotter.summed_plot(validation_result_merged, type='Validation')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'\\n---Test results--\\n')\n",
    "\n",
    "test_predictions, test_index = tft.predict(\n",
    "    test_dataloader, return_index=True, \n",
    "    show_progress_bar=arguments.show_progress\n",
    ")\n",
    "\n",
    "test_result_merged = processor.align_result_with_dataset(\n",
    "    test, test_predictions, test_index\n",
    ")\n",
    "show_result(test_result_merged, formatter.targets)\n",
    "plotter.summed_plot(test_result_merged, 'Test')\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_result_merged['split'] = 'train'\n",
    "validation_result_merged['split'] = 'validation'\n",
    "test_result_merged['split'] = 'test'\n",
    "df = pd.concat([validation_result_merged, test_result_merged])\n",
    "df.to_csv(os.path.join(plotter.figPath, 'predictions.csv'), index=False)\n",
    "\n",
    "print(f'Ended at {datetime.now()}. Elapsed time {datetime.now() - start}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
